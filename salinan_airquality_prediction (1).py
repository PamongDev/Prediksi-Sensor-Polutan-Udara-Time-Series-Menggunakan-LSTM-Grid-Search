# -*- coding: utf-8 -*-
"""Salinan AirQuality Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NbfBTslYdxpdl9hpDg372UWabl7mbmKJ

# Upload File Dataset  ☁
Upload Dataset dilakukan dengan menggunakan `google.colab` sehingga file dapat langsung diakses oleh notebook
"""

# prompt: upload file

from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

"""# Import Library 📒
Selanjutnya saya mengimport beberapa library yang saya gunakan, yaitu :

*   `numpy`
*   `pandas`
*   `matplotlib.pyplot`
*   `seaborn`
*   `sklearn.preprocessing`
*   `sklearn.model_selection`
*   `sklearn.metrics`
*   `tensorflow.keras.models`
*   `tensorflow.keras.layers`


"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, GRU

"""# Data Gathring 📁
Tahap ini, saya memasukan dataset yang telah di-upload, dan saya simpan dalam variabel `dataset`. Karena pada file `'AirQualityUCI.csv'` memiliki delimiter berupa `';'`, maka saya tambahkan parameter `delimiter` pada `pd.read_csv`
"""

dataset = pd.read_csv('AirQualityUCI.csv', delimiter=';')
dataset = dataset.drop(columns=['Unnamed: 15', 'Unnamed: 16'], errors='ignore')
dataset.head()

"""# Data Pre-processing ⛺
## Data Assesing & Data Cleaning 🔄
"""

dataset.info()

"""Dari `pandas.info` kita melihat bahwa,

* Dalam memprediksi `data Time Series Sensor Polutan`, tentu saya tidak menggunakan semua data. Data yang saya gunakan yaitu data pada sensor `PT08.S1(CO)`, `PT08.S2(NMHC)`, `PT08.S3(NOx)`,	`PT08.S4(NO2)`, dan `PT08.S5(O3)`.

* terdapat entri sebanyak `9471`, sedangkan data setiap column rata-rata sebanyak `9357`. Hal ini membuktikan terdapat missing Values sebanyak `9471 - 9357` atau `114` missing values.

* terdapat column yang berisi `0` entri sehingga perlu dihapus

"""

dataset.describe()

"""Dari `pandas.describe` kita melihat bahwa :

* terdapat nilai terkecil atau `min` dari setiap data yaitu bernilai `-200`. Hal ini perlu diatasi.
"""

data_object = dataset.select_dtypes(include=['object']).columns

dataset[data_object[2:]] = dataset[data_object[2:]].apply(lambda x: x.str.replace(',', '.')).astype(float)
dataset.head()

"""## Mengatasi Nan Values
Untuk mengatasi `Nan Values`, saya melihat jumlah dari setiap column terlebih dahulu.
"""

dataset.isna().sum()

"""Kemudian Saya menampilkan data `Nan Values` tersebut untuk melihat, apakah `Nan Values` ini perlu dihapus atau perlu diubah nilai nya."""

dataset[(dataset.isnull().any(axis=1))]

"""Karena `Nan Values` tersebut berjumlah sama dari setiap column nya dan tidak mengganggu data lainnya. Apabila saya `drop`, itu tidak masalah."""

dataset.dropna(inplace=True)
dataset.isna().sum()

"""## Mengatasi Anomali data (-200)
Ternyata terdapat banyak data yang bernilai `-200`. Tentu perlu dianalisis lebih lanjut, apakah kita `drop` barisnya atau kita `replace` dengan nilai lain.
"""

dataset[dataset==-200].count()

dataset[(dataset==-200).any(axis=1)]

"""Melihat dari tampilan diatas, nilai `-200` ternyata terdapat diantara data yang memiliki nilai dan acak. Jika dihapus barisnya, akan mempengaruhi proses `forcasting`. Dalam hal ini saya mengatasi nya sebagai berikut, yaitu :

* replace `-200` menjadi `Nan Values` dengan `np.nan`
* menggunakan metode `ffill` dan `bfill` pada `fillna`

Menggunakan cara tersebut membuat nilai `-200` ter-replace dengan nilai sebelumnya ataupun setelahnya.
"""

dataset_replace_nan = dataset.replace(-200, np.nan)

dataset_remove_nan = dataset_replace_nan.fillna(method='ffill').fillna(method='bfill')
dataset_remove_nan.info()

"""Data sudah bersih dari `missing values`"""

dataset_remove_nan[dataset_remove_nan==-200].count()

"""## Membuat Jam dan Tanggal menjadi Index

Saya melakukan ini agar data yang akan dilakukan pemodelan itu tidak tercampur dengan data waktu dan data tanggal.

* saya mengubah format `Time` pada umumnya, yaitu menggunakan `':'`.
* Saya juga mengubah format `Date` menjadi `%d/%m/%Y`
* Saya membuat kolom `datetime` yang nilainya merupakan gabungan dari column `Date` dan `Time` dan dijadikan index pada `dataset`
"""

dataset_remove_nan['Time'] = dataset_remove_nan['Time'].str.replace('.', ':')
dataset_remove_nan['datetime'] = pd.to_datetime(dataset_remove_nan['Date'] + ' ' + dataset_remove_nan['Time'], format='%d/%m/%Y %H:%M:%S')
dataset_remove_nan.set_index('datetime', inplace=True)
dataset_remove_nan.drop(['Date', 'Time'], axis=1, inplace=True)
dataset_remove_nan.head()

dataset_remove_nan.info()

"""# Cek Outlier ⛹"""

# Assuming dataset_remove_nan is your DataFrame
num_cols = len(dataset_remove_nan.columns)
num_rows = (num_cols + 4) // 5  # Calculate the number of rows needed

plt.figure(figsize=(20, 5 * num_rows))

for i, col in enumerate(dataset_remove_nan.columns):
    plt.subplot(num_rows, 5, i + 1)
    sns.boxplot(y=dataset_remove_nan[col])
    plt.title(col)

plt.tight_layout()
plt.show()

"""Berikut adalah analisis dari visualisasi boxplot untuk data sensor polutan :

1. **Outlier pada Data Time Series:**
   - Adanya outlier dalam data time series sering kali wajar, terutama pada pengukuran lingkungan seperti sensor polutan. Hal ini mungkin disebabkan oleh fluktuasi alamiah atau peristiwa yang tidak biasa seperti kebakaran, polusi tinggi, atau kegagalan alat.

2. **Data NMHC(GT):**
   - Pada data ini, terdapat **outlier yang sangat ekstrem**. Salah satu indikatornya adalah nilai anomali sebesar **-200**, yang berjumlah **8443 sampel dari total 9357 data**.
   - Nilai ini tampaknya merupakan **anomali teknis** atau **kesalahan pengukuran**, bukan pola wajar dalam data polutan.
   - memutuskan untuk **menghapus data ini**, yang merupakan langkah tepat karena nilai anomali tersebut bisa merusak model prediksi jika tidak ditangani.

3. **Data Sensor Lain:**
   - Outlier pada fitur lain terlihat relatif **wajar** dan berada dalam kisaran yang bisa dijelaskan oleh variasi normal. Misalnya:
     - **CO(GT), NOx(GT), NO2(GT):** Mungkin disebabkan oleh peningkatan aktivitas industri, lalu lintas, atau kondisi cuaca.
     - **RH, T:** Fluktuasi kelembapan dan suhu adalah hal yang umum pada data lingkungan.

Keputusan untuk tidak melakukan perbaikan pada `outlier` dan **fokus pada prediksi data time series merupakan pendekatan yang dapat dipertahankan karena model yang dibangun ingin menangkap pola alami dan bukan menormalkan data secara berlebihan.**

# EDA ⛅

# Univariate Analysis

Melakukan Analisis dari setiap data sensor
"""

dataset_remove_nan.hist(bins=50, figsize=(20,15))
plt.show()

"""Dari analisis setiap data, saya melihat data tersebut dengan merata, yaitu tidak `right-centris` atau `left-centris`.
Berikut analisis univariate untuk setiap fitur berdasarkan histogram yang Anda lampirkan:

1. **CO(GT):**
   - Distribusi data **positif skewed** (right-skewed).
   - Mayoritas data berada di rentang 0-4 ppm, menunjukkan konsentrasi karbon monoksida rendah mendominasi.
   - Nilai ekstrem hingga 12 ppm terlihat jarang.

2. **PT08.S1(CO):**
   - Distribusi menyerupai **normal**, dengan puncak sekitar 1000-1200.
   - Data ini menunjukkan pengukuran sensor CO relatif stabil, namun terdapat sedikit nilai ekstrem.

3. **NMHC(GT):**
   - Distribusi sangat aneh dengan **spike besar pada nilai -200** (outlier teknis).
   - Perlu perhatian lebih, karena spike ini tidak mencerminkan distribusi normal NMHC di atmosfer.

4. **C6H6(GT):**
   - Distribusi **positif skewed**.
   - Konsentrasi benzene rendah (0-20 µg/m³) mendominasi.
   - Beberapa nilai ekstrem mencapai 60 µg/m³ menunjukkan konsentrasi tinggi di waktu tertentu.

5. **PT08.S2(NMHC):**
   - Distribusi mendekati **normal**, dengan puncak sekitar 900-1200.
   - Konsistensi pengukuran sensor menunjukkan performa sensor yang stabil.

6. **NOx(GT):**
   - Distribusi **positif skewed**.
   - Mayoritas data berada di rentang rendah (0-300 ppb), menunjukkan tingkat oksida nitrogen yang rendah lebih sering terjadi.

7. **PT08.S3(NOx):**
   - Distribusi menyerupai **normal**, dengan puncak sekitar 600-1200.
   - Pengukuran sensor NOx tampak stabil tanpa adanya nilai ekstrem signifikan.

8. **NO2(GT):**
   - Distribusi data **positif skewed**.
   - Mayoritas data berada di rentang 0-100 ppb, menunjukkan bahwa konsentrasi NO2 rendah lebih sering terjadi.
   - Ada sedikit nilai ekstrem hingga 300 ppb.

9. **PT08.S4(NO2):**
   - Distribusi mendekati **normal**, dengan puncak sekitar 1500-2000.
   - Sensor menunjukkan data yang stabil tanpa banyak fluktuasi ekstrem.

10. **PT08.S5(O3):**
    - Distribusi menyerupai **normal**, dengan puncak di sekitar 800-1200.
    - Sensor menunjukkan stabilitas dalam mengukur ozon.

11. **T (Temperature):**
    - Distribusi **positif skewed**.
    - Suhu rendah mendominasi (sekitar 10-30 °C), dengan beberapa nilai tinggi hingga 40 °C.

12. **RH (Relative Humidity):**
    - Distribusi mendekati **normal**, dengan puncak di sekitar 40-60%.
    - Sebaran yang baik menunjukkan kelembapan relatif rata-rata.

13. **AH (Absolute Humidity):**
    - Distribusi mendekati **normal**, dengan nilai puncak sekitar 1.0-1.5 g/m³.
    - Sebaran ini mencerminkan variasi alami kelembapan absolut.

### Kesimpulan
- Sebagian besar fitur memiliki distribusi yang masuk akal untuk data lingkungan, dengan beberapa outlier yang wajar.
- **Fitur NMHC(GT)** perlu perhatian khusus karena anomali besar (-200). Langkah penghapusan data ini tepat dilakukan untuk meningkatkan kualitas dataset.
- Pola distribusi yang terlihat bisa membantu dalam pra-pemrosesan

# Multivariate Analysis
Melakukan analisis dengan menghubungkan setiap data
"""

sns.pairplot(dataset_remove_nan, diag_kind = 'kde')

"""### Korelasi Antara Variabel
1. **CO(GT) dan PT08.S1(CO):**
   - Terlihat hubungan **positif linier yang kuat**. Ini logis karena PT08.S1 adalah sensor CO, sehingga mengukur CO secara langsung.

2. **CO(GT) dan C6H6(GT):**
   - Hubungan **positif linier**, meskipun tidak sekuat CO(GT) dengan PT08.S1(CO). Ini menunjukkan bahwa konsentrasi karbon monoksida cenderung berkorelasi dengan benzene di udara.

3. **PT08.S2(NMHC) dan NMHC(GT):**
   - Hubungan **positif linier yang kuat**, menunjukkan bahwa sensor PT08.S2 efektif dalam mendeteksi konsentrasi NMHC.

4. **NOx(GT) dan PT08.S3(NOx):**
   - Hubungan **positif linier**, yang mengindikasikan sensor PT08.S3 mendeteksi oksida nitrogen secara akurat.

5. **NO2(GT) dan PT08.S4(NO2):**
   - Hubungan **positif linier yang kuat**, mendukung validitas data sensor PT08.S4 dalam mengukur nitrogen dioksida.

6. **PT08.S5(O3) dan Ozon:**
   - Hubungan terlihat moderat, tetapi sedikit tersebar, menunjukkan bahwa PT08.S5 efektif mengukur ozon namun mungkin ada noise pada data.

7. **T (Temperature) dan AH (Absolute Humidity):**
   - Hubungan **positif linier**, menunjukkan bahwa suhu yang lebih tinggi cenderung berasosiasi dengan kelembapan absolut yang lebih tinggi.

8. **T (Temperature) dan RH (Relative Humidity):**
   - Hubungan **negatif**, menunjukkan bahwa saat suhu meningkat, kelembapan relatif cenderung menurun.

9. **CO(GT) dan NOx(GT):**
   - Hubungan **positif**, meskipun ada beberapa penyebaran. Ini mengindikasikan hubungan tidak langsung antara karbon monoksida dan oksida nitrogen.

10. **NOx(GT) dan NO2(GT):**
    - Hubungan **positif linier**, menunjukkan bahwa peningkatan oksida nitrogen juga terkait dengan peningkatan nitrogen dioksida.

Melalui analisis Multivariate perlu untuk melihat korelasi data setiap fitur. Saya menggunakan metode `spearman` untuk mengetahui korelasi data setiap fitur.

## Data Correlation
"""

plt.figure(figsize=(20, 13))
correlation_matrix = dataset_remove_nan.corr(method='spearman')
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt=".2f", linewidths=0.5)

# Menampilkan plot
plt.title("Spearman Correlation Matrix Heatmap")
plt.show()

"""Dari heatmap korelasi menggunakan metode Spearman, kita dapat mengamati hubungan antar-variabel. Berikut adalah analisis mendalam berdasarkan heatmap tersebut:

1. **Korelasi Negatif Tinggi**:
   - Hubungan negatif yang signifikan dapat dilihat antara:
     - `PT08.S3(NOx)` dengan `C6H6(GT)` (~-0.85)
     - `PT08.S3(NOx)` dengan `PT08.S2(NMHC)` (~-0.85)
     - `PT08.S3(NOx)` dengan `PT08.S1(CO)` (~-0.86)
   - Korelasi negatif yang kuat ini mengindikasikan bahwa kenaikan satu variabel diikuti oleh penurunan pada variabel lain.

2. **Korelasi Rendah**:
   - `NMHC(GT)` memiliki korelasi rendah terhadap sebagian besar variabel, menunjukkan bahwa gas tersebut tidak terlalu dipengaruhi oleh atau memengaruhi variabel lain dalam dataset.

### Hasil analisis
Berdasarkan analisis yang dilakukan, penulis telah melakukan pemilihan fitur secara selektif untuk meningkatkan akurasi model prediksi. Fitur `PT08.S3(NOx)` yang memiliki `Korelasi Negatif Tinggi` dengan fitur lainnya dan fitur `NMHC(GT)` yang mengandung nilai outlier ekstrem (anomaly -200 sebanyak 8443 dari 9357 data) dihapus dari dataset. Langkah ini penting untuk menghindari distorsi pada model, karena fitur-fitur yang tidak relevan atau mengandung anomali dapat menurunkan kemampuan model dalam menangkap pola yang sebenarnya.

# Pemilihan Fitur ⛳
Berikut adalah hasil dari menghapus fitur `PT08.S3(NOx)` dan `NMHC(GT)` pada dataset
"""

low_correlation = ['NMHC(GT)', 'PT08.S3(NOx)']
dataset_remove_nan.drop(low_correlation, axis=1, inplace=True)
dataset_remove_nan.head()

"""
## Normalisasi data
Normalisasi pada data time series dengan 11 fitur beragam bertujuan untuk memastikan setiap fitur memiliki kontribusi yang seimbang dalam proses pembelajaran model, terutama ketika skala fitur berbeda-beda. Dengan menyelaraskan nilai ke rentang tertentu, seperti [0, 1]. Berikut adalah hasil dari normalisasi data dengan MinMaxScaler"""

from sklearn.preprocessing import MinMaxScaler

# Inisialisasi MinMaxScaler
scaler = MinMaxScaler()

# Fit dan transform data numerik
normalize = scaler.fit_transform(dataset_remove_nan)

# Tampilkan data yang sudah dinormalisasi
dataset_normalize = pd.DataFrame(normalize, columns=dataset_remove_nan.columns, index=dataset_remove_nan.index)
dataset_normalize.head()

"""# Windowing
Windowing adalah sebuah proses untuk mengkonversi data `time-series` menjadi data dengan jarak waktu tertentu sehingga menentukan `data fitur` dan `data target`
"""

def create_windows_multivariate(data, window_size, output_size, stride):
    X, y = [], []
    for i in range(0, len(data) - window_size - output_size + 1, stride):
        X.append(data.iloc[i:i + window_size].values)  # input sequence
        y.append(data.iloc[i + window_size:i + window_size + output_size].values)  # output sequence
    return np.array(X), np.array(y)

x, y = create_windows_multivariate(dataset_normalize, 10, 1, 1)
x.shape, y.shape

print('data fitur= ',x[0])
print('target = ', y[0])

dataset_normalize.head(11)

"""Jika melihat dari hasil `windowing` dengan dataframe `dataset`, waktu ke-10 pertama itu merupakan `data fitur`, waktu ke-11 adalah `data target`

## Splitting Data
Pada data time series, membagi dataset menjadi 80:20 tanpa menggunakan shuffle dan random state sangat penting untuk menjaga urutan temporal data. Data time series memiliki sifat berurutan di mana nilai masa depan bergantung pada pola di masa lalu. Oleh karena itu, saya **menonaktifkan shufle pada saat splitting data**.
"""

# Split data into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, shuffle=False)
X_train.shape, X_val.shape, y_train.shape, y_val.shape

"""# Model ⏰

Pada tahap modelling, penulis menggunakan Metode LSTM dalam memprediksi sensor polutan yang memiliki multi fitur. Ada 2 algoritma yang diterapkan, yaitu LSTM dan GRU. Kedua algoritma ini akan menerapkan grid search sehingga dapat memperoleh performa yang optimal.

**Parameter yang akan dioptimalkan oleh Grid Search yaitu, Learning rate dan Optimizer.**
## Learning Rate:
-	Learning rate menentukan seberapa besar langkah yang diambil model dalam memperbarui bobotnya selama pelatihan.
-	Jika learning rate terlalu besar, model mungkin gagal mencapai konvergensi, sedangkan learning rate yang terlalu kecil membuat pelatihan lambat atau terjebak dalam lokal minima.
-	Melakukan grid search pada learning rate membantu menemukan nilai optimal yang memungkinkan model belajar secara stabil dan efisien.
-	Learning rate yang diujikan yaitu `0.001` dan  `0.0001`

## Optimizer:
-	Optimizer bertanggung jawab untuk mengatur cara model memperbarui bobot berdasarkan gradien.
-	Berbagai optimizer, seperti Adam dan RMSprop, memiliki karakteristik berbeda dalam menangani data dengan pola nonlinier atau noise.
-	Grid search pada optimizer memungkinkan untuk menentukan algoritma yang paling cocok dengan dataset dan arsitektur model, sehingga meningkatkan akurasi dan stabilitas pelatihan.
-	Optimizer yang diujikan yaitu `Adam` dan `RMSprop`

## Output Model
-  Dari hasil Grid Search, output yang dihasilkan adalah model terbaik dan kombinasi parameter terbaik.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, GRU
from tensorflow.keras.optimizers import Adam, RMSprop
from sklearn.model_selection import train_test_split

def build_lstm_model(input_shape, output_shape, learning_rate, optimizer):
    model = Sequential([
        LSTM(32, activation='tanh', input_shape=input_shape),
        Dense(output_shape, activation='linear')
    ])

    # Compile model with given optimizer and learning rate
    if optimizer == 'adam':
        opt = Adam(learning_rate=learning_rate)
    elif optimizer == 'rmsprop':
        opt = RMSprop(learning_rate=learning_rate)
    else:
        raise ValueError("Unsupported optimizer")

    model.compile(optimizer=opt, loss='mse', metrics=['mse'])
    return model

def build_gru_model(input_shape, output_shape, learning_rate, optimizer):
    model = Sequential([
        GRU(32, activation='tanh', input_shape=input_shape),
        Dense(output_shape, activation='linear')
    ])

    if optimizer == 'adam':
        opt = Adam(learning_rate=learning_rate)
    elif optimizer == 'rmsprop':
        opt = RMSprop(learning_rate=learning_rate)
    else:
        raise ValueError("Unsupported optimizer")
    model.compile(optimizer=opt, loss='mse', metrics=['mse'])
    return model

def grid_search_model(X_train, X_val, y_train, y_val, input_shape, output_shape, model_name,learning_rates, optimizers, epochs=20, batch_size=64):
    best_loss = float('inf')
    best_params = None
    best_model = None

    # Grid search over learning rates and optimizers
    for lr in learning_rates:
        for opt in optimizers:
            print(f"Training {model_name} with learning_rate={lr}, optimizer={opt}")
            if model_name == 'lstm':
                model = build_lstm_model(input_shape, output_shape, lr, opt)
            elif model_name == 'gru':
                model = build_gru_model(input_shape, output_shape, lr, opt)
            else:
                raise ValueError("Unsupported model")

            # Train the model
            history = model.fit(X_train, y_train,
                                validation_data=(X_val, y_val),
                                epochs=epochs,
                                batch_size=batch_size,
                                )

            # Evaluate on validation data
            val_loss = history.history['val_loss'][-1]
            print(f"Validation loss: {val_loss}")

            # Update best parameters if current loss is lower
            if val_loss < best_loss:
                best_loss = val_loss
                best_params = {'learning_rate': lr, 'optimizer': opt}
                best_model = model

    print(f"Model {model_name} : Best params: {best_params} with loss: {best_loss}")
    return best_model, best_params

# implementation method
learning_rates = [0.001, 0.0001]
optimizers = ['adam', 'rmsprop']

input_shape = (x.shape[1], x.shape[2])
output_shape = y.shape[2]

best_model_lstm, best_params_lstm = grid_search_model(X_train, X_val, y_train, y_val, input_shape, output_shape, 'lstm', learning_rates, optimizers)
best_model_gru, best_params_gru = grid_search_model(X_train, X_val, y_train, y_val, input_shape, output_shape, 'gru', learning_rates, optimizers)

"""# Evaluation
Untuk evaluasi, penulis menggunakan MSE sebagai salah satu cara untuk menilai performa model prediksi, termasuk dalam kasus prediksi time series untuk sensor polutan. Mean Squared Error (MSE) mengukur rata-rata kesalahan kuadrat antara nilai prediksi  dan nilai aktual dari data.

## Hasil Training dan validasi
Berikut adalah hasil Training dan validasi antara Algoritma LSTM dan GRU.
"""

import matplotlib.pyplot as plt

def plot_loss(history):
    """
    Membuat grafik loss dari hasil training model.
    Args:
    - history: objek history dari model.fit().
    """
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Train Loss', color='blue', marker='o')
    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange', marker='o')
    plt.title('Training and Validation Loss', fontsize=16)
    plt.xlabel('Epochs', fontsize=14)
    plt.ylabel('Loss (MSE)', fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(alpha=0.3)
    plt.show()

plot_loss(best_model_lstm.history)

"""Terlihat bahwa Algoritma `LSTM` dapat belajar dengan baik."""

plot_loss(best_model_gru.history)

"""Selain itu, Algoritma `GRU` juga dapat meminimalisir error.

# Hasil perbandingan data actual dan data prediksi
## LSTM
"""

y_pred = best_model_lstm.predict(X_val)
y_actual = y_val.reshape(1870, 11)

plt.figure(figsize=(20, 25))  # Ukuran figure yang lebih besar untuk 11 subplots

sensor_names = dataset_remove_nan.columns.tolist()
for i in range(len(sensor_names)):
    plt.subplot(6, 2, i + 1)  # Subplots dalam grid 6 baris x 2 kolom
    plt.plot(y_actual[:, i], label="Actual", color='blue')
    plt.plot(y_pred[:, i], label="Predicted", color='orange', linestyle="--")
    plt.title(sensor_names[i], fontsize=14)  # Judul tiap grafik sesuai sensor
    plt.xlabel("Time", fontsize=12)
    plt.ylabel("Value", fontsize=12)
    plt.legend(fontsize=10)
    plt.tight_layout()

plt.show()

"""Dari grafik perbandingan antara data aktual (biru) dan data prediksi (jingga), terlihat bahwa model mampu menangkap pola umum pada beberapa fitur, seperti T dan AH, namun kurang efektif dalam merepresentasikan fluktuasi tajam pada fitur lain, seperti CO(GT) dan NOx(GT). Hal ini menunjukkan bahwa model memiliki keterbatasan dalam menangani outliers atau perubahan data yang cepat, meskipun cukup baik dalam mengenali tren jangka panjang. Kinerja model secara keseluruhan dapat ditingkatkan dengan optimasi hyperparameter, pengayaan dataset, atau eksplorasi algoritma tambahan untuk menangkap pola yang lebih kompleks.

## GRU
"""

y_pred = best_model_gru.predict(X_val)
y_actual = y_val.reshape(1870, 11)

plt.figure(figsize=(20, 25))  # Ukuran figure yang lebih besar untuk 11 subplots

sensor_names = dataset_remove_nan.columns.tolist()
for i in range(len(sensor_names)):
    plt.subplot(6, 2, i + 1)  # Subplots dalam grid 6 baris x 2 kolom
    plt.plot(y_actual[:, i], label="Actual", color='blue')
    plt.plot(y_pred[:, i], label="Predicted", color='orange', linestyle="--")
    plt.title(sensor_names[i], fontsize=14)  # Judul tiap grafik sesuai sensor
    plt.xlabel("Time", fontsize=12)
    plt.ylabel("Value", fontsize=12)
    plt.legend(fontsize=10)
    plt.tight_layout()

plt.show()

"""Grafik perbandingan data aktual dan prediksi menggunakan GRU untuk berbagai sensor polutan menunjukkan bahwa model GRU mampu menangkap pola umum dari data aktual, terutama pada tren jangka panjang. Namun, terdapat beberapa deviasi yang cukup signifikan, terutama pada fluktuasi mendadak, di mana prediksi sering kali lebih halus dan tidak sepenuhnya mengikuti lonjakan ekstrem dalam data aktual. Hal ini mengindikasikan bahwa GRU efektif dalam mempelajari pola temporal utama, tetapi kurang responsif terhadap perubahan nilai yang cepat, yang mungkin disebabkan oleh kurangnya informasi pada jendela waktu atau perlunya pengaturan hiperparameter lebih lanjut. Meskipun demikian, GRU tetap menjadi model yang layak untuk memprediksi data time series multivariabel seperti sensor polutan, dengan potensi peningkatan akurasi melalui optimalisasi lebih lanjut.

# Evaluasi Akhir
"""

best_model_lstm.evaluate(X_val, y_val)

best_model_gru.evaluate(X_val, y_val)

"""Dari hasil evaluasi akhir tersebut menunjukan bahwa **LSTM memiliki performa yang lebih baik dibandingkan dengan GRU**.

# Summary Arsitektur Model
"""

best_model_lstm.summary()

best_model_gru.summary()